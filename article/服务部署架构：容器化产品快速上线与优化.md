## 1.问题背景

在这个降本增效的大环境下，企业创新和研究型投入逐年下滑，转而对快速上线，提升人效和资金周转效率的需求增加，个人的技术水平除了深度之外，对于广度的指标（能否支持从需求分析到开发再到上线部署），成为又一个较为苛刻的技术趋势。本文简述部分可以支撑小企业或者个人需求的交付技术。适合有以下境遇的人群阅读：

**全栈&个人开发**
这是一个纯作坊式或者极客式的需求，此类人群往往有一股浓厚的贫穷味道，对产品的“存在”格外在意，其他部分则不甚关心。只要有一个可以在面试、融资的时候讲故事的可执行程序即可。工作流包括但不限于远程桌面传输动态库、云盘压缩存代码、虚拟机部署服务、更改缩放比例适配高分辨率屏幕等等……

**小企业快速项目上线**
此类需求往往来自老板，或者某个大厂跳槽进驻的项目经理。项目总包可能从几十万到几千万不等，但是利润极其微薄，时间极其紧迫，主打一个敏捷开发，上午讲的PPT，下午拉起项目会，晚上正式开始开发，下周要有demo上线。工作流可能包括飞书文档、诡异的jira流程、github下载zip源码、淘宝发起采购流程、前后矛盾的客户需求、以及为了临时功能开辟的50个git分支……

**大企业内部项目**
此类需求的环境好于上两种，但是也好的不太明显，由于有限的预算与人手，极低的价值创造以及可有可无的存在感，导致开发频繁被打断或者中止、服务环境由于产线需求频繁被挪动、莫名其妙的适配需求以及随时改变的预期形态、还有中期对于产品价值的灵魂拷问，需要开发者心态和身段技术如同猫一样灵活……

以上是大部分所谓的工程师、软件专家的日常生存环境，为了在此等环境中，尽可能满足所有甲方的需求，同时不至于把自己累死，我们需要一套兼顾灵活轻量和可开发性的技术栈，以应对包括需求分析（本文不做讲解，请挪步服务架构、客户端架构篇）、设计分层、整体打包、环境迁移、性能优化、运行维护、扩容均衡等常见工况。

## 2.解决方案

1. **应用分级分层设计**
对于现代软件而言，单体巨无霸式的设计明显是不符合时代需求的，按照一个基础的可维护的设计框架，我们将需要部署和维护的软件分为下面的层级：应用层、接口转发层、通信层、算法层、数据层、硬件层。

由于我们主打一个灵活部署，持续更新，所以我们也没有必要按照大厂的方式细分，而是将【应用层、接口转发层、算法层】合并为应用服务层；将通信层独立为中间件层；数据层独立为数据库层；硬件层暂时不做考虑。

针对不同的层级，我们需要选择不同的技术栈，我这里给出一个c++的常见选择（请移步服务端架构篇），对于java、rust、go等其他主流预研开发者，可以选择自己熟悉的方式。



2. **发布与整体打包**
整体的发布与打包分为两个阶段，一个是我们自己开发的部分，如一个行业算法库、一个tcp服务、或是一个业务场景；另一部分是整合应用服务、通信中间件、数据库作为整体解决方案包。

应用服务层的打包主要由于功能繁杂，三方库和依赖多而且复杂，所以我们选择整体打包容器化的方式，使用基础镜像，例如ubuntu，安装基础依赖，然后将需要源码编译的部分单独作为挂载对象搭配容器使用。至于中间件层、数据库层则有成熟的第三方整体解决方案，我们可以使用docker-compose的容器编排（针对小规模业务）、docker swarm的服务组合（针对中等规模的业务）、k8s的服务编排（针对有一定体量的业务）整体搭建完整的服务体系。

将不同的docker镜像打成tar包，个人或公司开发的应用服务单独打包，均上传至个人或者公司FTP服务器，使用编排脚本管理，实现一键式下载、环境搭建、服务启动与应用初始化。针对客户内网的情况，tar包加脚本的形式也可以装载于硬盘中，通过客户的保密审查后，直接加载到客户的服务器上。由于应用采取目录挂载的形式，可以单独进行升级，也可以通过NFS等手段，隔离不会变更版本的基础设施与定期更新的服务应用。


3. **环境迁移**
由于采用容器化服务的方式，整体的解决方案并不依赖特定的平台、硬件、系统；使用时需要在目标服务器上安装需要的docker、docker-compose、swarm、k8s等平台，然后将服务整体的解决方案包导入加载即可


4. **运行维护**
运行维护主要分为资源监控、应用升级、日志收集、故障排查、恢复重启这几个部分。

资源监控：针对应用部分，我们推荐使用k8s的组件或者自定义脚本（无管理平台时，通过抓取htop数据）实现监控功能，由于主要针对小型业务，我建议在内部模拟完压力场景后，在上线初期开启脚本或者组件监控，其余时间关闭这个功能。针对中间件和数据库，建议选择自带控制台的版本，直接通过网页控制台处理。

应用升级：针对中间件与数据库，我们建议直接使用成熟版本，不进行升级，在漫长的业务生命周期中，人员的变动往往比三方库更快，所以请在服务稳定之后，仅升级自己公司开发的部分。也就是更新我们上文提到的，单独业务包，如果你希望实现热切换，则可以创建多个挂载路径，使用编排平台的功能实现。

日志收集、故障排查：建议单独使用NFS处理这两个部分，主要是需要脱离容器持久化处理，日志文件可以设置滚动更新，仅保留固定大小和数量的日志，多数日志库都有这一功能，三方平台与软件的日志则通过设置NFS路径作为输出路径将文件拷贝出来放置。故障排查建议使用dump留存加上崩溃时输出调用堆栈到日志文件的处理方式，前者保留现场，后者提供快速定位的入口，特别是多数崩溃会引来急急国王，一个可以快速传输和阅读的调用堆栈日志可以给你一个底。

恢复重启：如果没有使用K8S等平台，重新恢复启动就可以简单的把所有容器使用docker-compose down时候重新up，数据库中的数据会留存，服务短暂断开；如果使用了编排平台，请使用滚动恢复机制。

5. **扩容与负载均衡**
这一条是针对swarm集群和k8s集群的，我个人的建议是不要把摊子摊太大，先考虑人手，再选择技术，如果开发兼任运维，且可投入人力小于5人，千万不要头铁上k8s，各种看似高级的特性一用一个不吱声。而针对扩容，小规模业务场景，抛开数据服务器与文件服务器，一般用于应用的应该不会超过十台，可以直接考虑在启动脚本中设置好，有状态的就一台服务器上一个，无状态的就定好需要的数量让swarm或者k8s自己去分配。

而负载均衡，建议直接用nginx做代理与转发，它足够简单通用，多数人都会使用，而且功能与性能也基本满足这个规模下的所有场景。

总之，配置越简单，服务越可靠。

## 3.性能优化

**应用性能优化**
请参考服务端、客户端架构

**IO性能优化**
请参考服务端、客户端架构

**中间件性能优化**
消息队列的并发优化
我们以kafka为例：

异构消息管线
通信的评判标准和优化方向主要就两个：可靠性和传输效率
不同的通信原理会导致通信评价天差地别，不同的使用环境需要使用不同的技术，学习过计算机基础的同学都知道，通信方式可以用共享内存、管道、网络
落到工程应用中，通常情况下，我们都不建议自己重新造轮子，所以我们以zmq+fastdds+kafka+mqtt为例。
在自动驾驶领域，本机之间的通信主要要求是实时性，所以首选的是共享内存，fastdds原生支持共享内存的通信，无疑是单机上的首选



共享内存-局域网（跨机器）-公网（跨域）

**算法性能优化**
请挪步性能优化篇

vtune热点函数
bpf边车测试

**数据库性能优化**
sql优化

## 4.故障排除

**单机gdb、windbg调试**
gdb单机调试只需要掌握一些基础的命令即可，详情请参考博客：

**docker调试**

**bpftrace调试**
请移步：[bpf基础使用指南](https://www.zhaolingxi.com/archives/bpftrace%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97)

**日志性能记录**
这里主要采取bpftrace+软件日志的方式进行记录。
bpftrace的内容参见：
软件日志的内容参见三方库推荐（日志部分）