## 1.背景与前言：
从某种传统的分类来说，程序员可以简单的划分为前后端，前端负责交互、展示，后端负责数据、计算。无论是早期的客户端界面与后台程序、还是后来流行的浏览器与远端服务、亦或是某些软件的云服务+本地混合计算的结构，后台软件始终以一种不间断的服务提供商的身份支撑业务运行；作为经验总结的一部分，本篇将阐述基础服务端的层级、架构、建设、维护，作为初入服务端的指导。

## 2.业务模块整体架构设计
服务总体可以简化设计为：

秉承着工程化的思想，我们并不建议在每一层自己重新造轮子，封装线程协程、自造数据库或者手搓搜索统计算法，都不在我们讨论的范畴，我们主要会聚焦于如何站在开源巨人的肩膀上，将我们的业务搭建运行起来。

## 3.分层级技术路线
#### 驱动层：
所谓驱动在硬件层面的定义是：嵌入操作系统的有关硬件设备的信息程序。 此信息能够使计算机与相应的设备进行通信。我们这里指的是软件如何随着时间向前运行，常见的驱动方式有时间驱动（定时器）以及事件驱动（IO通知、调用通知等）。在软件层面，我们有很多种设计实现方式，包括epoll/IOCP/io_uring等等，我们这里不做一些设计模式上的争论，而是**选择一个成熟的异步运行时（runtime）**，让你的服务端程序能够在时间推进中持续运转、响应事件、调度任务、控制并发，并为上层（数据层/算法层/接口业务层）提供稳定一致的执行语义。鉴于我个人对于跨平台和省心的偏好，我这里以ASIO standlone作为驱动设计的蓝本。

驱动层主要把“服务端程序的持续运行”拆成 4 类可控的事情：
1. **事件驱动**：网络读写就绪、连接到来、DNS 解析完成、信号触发等。
2. **时间驱动**：定时任务、超时控制、重试退避（backoff）、心跳、过期回收等。
3. **并发调度**：多线程并行处理 I/O 与轻量逻辑，避免锁争用与竞态。
4. **任务编排**：把上层业务逻辑以“任务”的形式投递到合适的执行上下文中运行（避免阻塞事件循环）。

在使用层面上，我们核心的工具有三个
1.并发线程池+io context上下文
相信学习过C++的同学都实现过多线程模型，不过在生产环境用，用手搓的简易线程池显然不够让人安心，asio封装了epoll和iocp的调度模式，我们只需要创建多个线程，把他们统一加入io_context，并且调用run，后续就不用再维护，只需要提交任务即可，asio会自己帮我们执行。


2.定时器timer（处理定时触发与超时控制）
定时任务，是系统中最常见，也是最好理解的任务执行方式，例如传感器上报信息、计算线程轮询接口等场景下，定时定频触发通常依赖于定时器，即硬件晶体时钟的定时消息发送。在时间触发的场景下，我们需要处理的情况分为两种：1.定时触发程序；2.超时异常处理。


3.多strand+post任务提交
不是所有的任务都是独立、互不影响的，在很多时候我们需要任务按照一定的依赖顺序执行，这个时候就需要strand出场了，它能保证我们post上去的任务按照我们post的顺序执行，可能在直觉上这个很简单，但是在并发场景下，此时会有很多的麻烦需要处理，这也是asio省心的地方。

我们这里用asio 1.30作为讲解蓝本，不同版本的策略可能会不一样
asio里有io_context,一个io_context对应一个excutor，每一个io_context有一个全局的FIFO的任务队列（实际的实现加入了多层优化），所有的普通任务提交都到这里，然后按序执行，一个io_context共享一个线程池，每个线程都在调用run，每次run的时候，每个线程都会在全局队列没有写入的时候（锁竞争）按照任务插入顺序获取一个任务执行，普通任务立即执行，如果这是一个strand任务，就会有两种情况，一种是当前strand有任务正在跑，不知道需要多长时间，那就在提交的时候会提交到strand管理的等待队列中，这种任务每次执行完，就会把下一个的同属于一个strand的任务提交到就绪的队列中，如果当前strand本身就没有任务再运行，就直接推到就绪队列，然后整个strand去排队当作普通任务执行；多个io_context独立管理自己的任务和线程池。
需要注意的是这里有的教材会说asio的strand会采取内循环执行的策略，但是据我的观察，其源代码中为了避免长期占用线程，会重新投递和分配任务，代码如下：
```
std::size_t scheduler::do_run_one(mutex::scoped_lock& lock,
    scheduler::thread_info& this_thread,
    const asio::error_code& ec)
{
  while (!stopped_)
  {
    if (!op_queue_.empty())
    {
      // Prepare to execute first handler from queue.
      operation* o = op_queue_.front();
      op_queue_.pop();
      bool more_handlers = (!op_queue_.empty());

      if (o == &task_operation_)//strand入口
      {
        task_interrupted_ = more_handlers;

        if (more_handlers && !one_thread_)
          wakeup_event_.unlock_and_signal_one(lock);
        else
          lock.unlock();

        task_cleanup on_exit = { this, &lock, &this_thread };
        (void)on_exit;

        // Run the task. May throw an exception. Only block if the operation
        // queue is empty and we're not polling, otherwise we want to return
        // as soon as possible.
        task_->run(more_handlers ? 0 : -1, this_thread.private_op_queue);
      }
      else
      {
        std::size_t task_result = o->task_result_;

        if (more_handlers && !one_thread_)
          wake_one_thread_and_unlock(lock);
        else
          lock.unlock();

        // Ensure the count of outstanding work is decremented on block exit.
        work_cleanup on_exit = { this, &lock, &this_thread };
        (void)on_exit;

        // Complete the operation. May throw an exception. Deletes the object.
        o->complete(this, ec, task_result);
        this_thread.rethrow_pending_exception();

        return 1;
      }
    }
    else
    {
      wakeup_event_.clear(lock);
      wakeup_event_.wait(lock);
    }
  }

  return 0;
}
```

#### 数据层
在数据层，我们主要需要设计两类数据的存储，高频使用的内存数据、以及需要持久化和不频繁参与计算的数据库数据。
这里，我们将使用三个数据库作为核心工具：sqlite、postgresql、clickhouse，他们分别对应的场景是：单机简单服务、传统客户信息存取、统计服务
我们这里不去讲解数据库的实现，而是聚焦于怎么用数据库。
首先，我们聚焦与数据本身，常见的数据我们可以分为数值（整数、小数）、文本、二进制；如果我们不考虑性能，只考虑兼容性，可以简化为uint64_t \ double \ string \ blob。
不同的数据库支持的数据在细节上有差异，如果要接入多个数据库就需要转换，也就是说，再数据库和上层应用之间，需要一个通用的数据兼容转换层。我们给出一个简单的伪代码示例

```
enum class DbType {
    Null,
    Int64,//可以用来表示int64 时间戳
    Uint64_t,
    Double,
    String,//可以用来表示string 时间戳
    Blob,
    Bool,
};

struct DbValue {
    using Blob  = std::vector<std::uint8_t>;
    using Value = std::variant<std::monostate, std::int64_t, std::uint64_t, double, std::string, Blob, bool>;

    Value v;

    // 判空
    bool is_null() const { return std::holds_alternative<std::monostate>(v); }

    // 构造辅助（从常用类型）
    DbValue() : v(std::monostate{}) {}
    DbValue(std::nullptr_t) : v(std::monostate{}) {}
    DbValue(std::int64_t x) : v(x) {}
    DbValue(std::uint64_t x) : v(x) {}
    DbValue(double x) : v(x) {}
    DbValue(bool x) : v(x) {}
    DbValue(std::string s) : v(std::move(s)) {}
    DbValue(const char* s) : v(std::string(s ? s : "")) {}
    DbValue(Blob b) : v(std::move(b)) {}

    static DbValue null() { return DbValue{}; }
};
```

这里的还需要做数据的类型判定与转换，我们就不再赘述。
除了数据兼容，我们也需要对常见的语义、常见的执行方式做简单抽象，对于数据库而言，业务无外乎就是增删改查，执行方式分为立即执行与事务执行，针对这两个部分，我们需要做简单的接口定义：
```
class DatabaseBackend {
public:
    virtual ~DatabaseBackend() = default;
    
    // 连接管理
    virtual bool connect(const std::string& conn_str) = 0;
    virtual void disconnect() = 0;
    virtual bool ping() = 0;
    
    // 执行SQL（同步）
    virtual int64_t execute(const std::string& sql) = 0;
    virtual int64_t execute(const std::string& sql, const std::vector<DbValue>& params) = 0;
    
    // 查询（返回字符串字典，通用格式）
    virtual ResultSet query(const std::string& sql) = 0;
    virtual ResultSet query(const std::string& sql,
                            const std::vector<DbValue>& params) = 0;
    
    // 事务（PostgreSQL/SQLite支持，ClickHouse不支持）
    virtual void begin_transaction() = 0;
    virtual void commit() = 0;
    virtual void rollback() = 0;
    virtual bool supports_transactions() const { return true; }
    
    // 批量操作
    virtual int64_t execute_batch(const std::string& sql, 
                                   const std::vector<std::vector<DbValue>>& params_list) = 0;
    
    // 获取最后插入ID
    virtual int64_t last_insert_id() const = 0;
    
    // 错误信息
    virtual std::string error_message() const = 0;
    
    // 线程安全连接获取（用于thread_local）
    virtual void* get_thread_local_connection() = 0;
    virtual void release_thread_local_connection(void* conn) = 0;
};
```
具体的数据库可以继承基础接口后扩展，并在实现类中针对不同数据库特性扩展基础的增删改查语句。

在大一些的项目中，不同的数据库之间也是有层级的，比如典型的redis做缓存，导入后续的分析型和关系型数据库，我们这里也不做展开。


#### 算法层：
以我的从业方向来说，常见算法主要集中于几何、约束求解、以及数理统计方面，所以，我们需要的工具有：eigen、proj、or-tools、gsl、clipper、cgal……等等。
这是一个千人千面的层级，在不同行业、不同公司、不同岗位上，算法层的结构和内容千差万别，我们这里以一个港口自动驾驶领域可能涉及到的内容举个例子。模拟一个云端感知到决策避障的过程，具体的逻辑包括：接收单车传感器融合数据、定位数据、模型数据，转换为轮廓并映射到地图，接收车辆的运行规划，进行碰撞检测、运行偏离检测，在异常情况下，暂停所有车辆，规划调度所有车辆驶入停车区的过程。
我们先将高精地图解析为纯粹的三角面片（地图、平面几何），然后用顶点融合的方式划分为不同的路径区块，并与高精地图的道路信息做好双向映射，形成一张虚拟地图（数据结构优化）；所有的车辆轮廓存储于云端数据库，车辆定位上线时在虚拟地图中创建车辆二维形体（平面几何）；车辆由自车或者云端发送指令，执行预定的装卸行驶轨迹，实时上报运动状态信息，云端执行实时的碰撞预测（运动学），紧急情况时，按区域暂停任务；云端暂定区域的车辆将未完成的任务重新规划路径，结合车辆速度生成带时空信息约束的任务，进行约束求解（约束求解），将求解结果下发车辆，重新启动任务。


#### 接口业务层：
接口业务分为两个方向，一个是面对集群业务的内部接口，常见于数据流，例如kafka、fastdds、mqtt的数据传输接口，以及内部回调，一种则是面向用户，或者下游调用方的传统服务接口，作为服务端，我们需要暴露不同格式的调用接口，常见的调用方式主要分为直接调用（c-api）、跨语言调用（常见于前端调用算法，例如JS调用C++）、网络接口调用（基于http、websocket、tcp、udp的业务接口），以及可能的简易状态机设计与处理。
分块来说：

信息流包装接口：
对于kafka\fastdds\mqtt以及其他一些注重于信息传输本身的协议，我们通常是不会在连接、qos管理等等其原生支持的功能上面动手脚，而是使用一个封装帧将内容装进body中，将需要携带的id、时间戳、数据类型等封装进header中，用json或者protobuf打包后作为数据帧传输，此时我们采用原生的kafka 、mqtt 的client和server完成收发，并且提供数据帧接口协议文档，按照数据帧协议完成业务逻辑闭环。

网络调用接口：
网络调用是最常见的远程请求，其中有高层级的调用方式，例如http或者长连接的ws，也有低层级的tcp和udp，一般来说，我们会把命令包装到json等格式，使用http做请求，解析请求之后，服务端调用计算模块计算完成后返回，如果需要常会话，就会采用ws，如果更偏向于数据的持久传输，则会使用tcp或者udp，而tcp与udp的数据收发一般情况下就会更加偏向于二进制的传输和解析，例如把数据封装到协议之后的C结构体中，或者使用protobuf打包，一般情况下，网络接口调用需要自己写服务端server代码处理数据业务调用，如果是高层级的，可以借助crow等网络协议三方库来解决解析、连接管理等问题，如果是纯tcp，还需要自行处理帧解析和多连接管理。

跨语言调用接口：
跨语言调用的场景主要存在于前端调用算法，由于浏览器的性能限制以及js等语言的开发局限，前端点击后需要后端调用c++的算法，此时就需要借助一些额外的调用框架来辅助实现，常见方式包括使用wasm的编译算法包，或者使用koffi等跨语言调用框架直接调用c的api达成目的。


## 4.性能优化与扩容
对于单机的优化方向，主要在多线程并发、热点函数处理、调试方向。
通常情况下，我们会结合程序日志（log文件）、热点函数分析（perf、intel implfer、bpftrace）、运行态调试(gdb、windebug)、资源监控来综合分析，优化的重点部分还是我们自己的业务代码，所以我们这里以热点函数分析为例。
我们这里选择bpftrace，其轻量、灵活、低消耗的特性让我在一般场合几乎已经抛弃了perf，当然这里不展开对比这些工具，我们说一下它们的基础使用逻辑。
几乎所有的这类工具，本质意义上都是定频采样，比如一秒钟1000次，在采样的过程中，如果某个函数一直都在，就意味着它一直在运行，就意味着CPU的高占用。我们这里采用sidecar的启动方式，即使用一个容器启动bpftrace，利用它采集信息，然后生成可视化分析图。整体的部署包结构如下：

用户栈+系统栈聚合火焰图
```
      sh -c "
        echo 'Profiling PID='$$TARGET_PID;
        bpftrace -e '
          profile:hz:99 /pid == $$1/ {
            @[ustack(20), kstack(20)] = count();
          }
          interval:s:10 {
            print(@);
            exit();
          }
        ' $$TARGET_PID > /data/profile.raw 2>&1
      "
```

系统调用延迟直方图：`read()` 延迟，
```
sudo TARGET_TGID=1234 bpftrace -e '
tracepoint:syscalls:sys_enter_read
/pid == strtol(env("TARGET_TGID"))/
{
  @start[tid] = nsecs;
}

tracepoint:syscalls:sys_exit_read
/@start[tid]/
{
  $d = nsecs - @start[tid];
  @read_us = hist($d / 1000);
  delete(@start[tid]);
}

interval:s:10 { print(@read_us); clear(@read_us); exit(); }
'
```

调度延迟/运行队列等待：从 wakeup 到真正运行的时间，衡量“线程被唤醒后，在 runqueue 等了多久才上 CPU”。
```
sudo TARGET_TGID=1234 bpftrace -e '
tracepoint:sched:sched_wakeup
/args->pid != 0/
{
  // 这里只能拿到被唤醒线程的 tid（args->pid）
  // 先记时间，等它真正被切上 CPU 时再算差
  @wakeup_ts[args->pid] = nsecs;
}

tracepoint:sched:sched_switch
/@wakeup_ts[args->next_pid]/
{
  // next_pid 是即将运行的 tid
  $d = nsecs - @wakeup_ts[args->next_pid];

  // 过滤：只有当这个 tid 属于目标进程时才统计
  // bpftrace 这里拿不到 next 的 tgid，只能用 comm/已有标记等手段；
  // 最简单：只统计“我们关心的线程”——先在 switch_out 时标记它（见 Off-CPU 那个脚本更严谨）。
  @runq_us = hist($d / 1000);

  delete(@wakeup_ts[args->next_pid]);
}

interval:s:10 { print(@runq_us); clear(@runq_us); exit(); }
'
```

块 IO 延迟：从请求下发到完成（block layer），这能告诉你磁盘 IO 慢不慢（从 block 层视角），不需要指定 PID。
```
sudo bpftrace -e '
tracepoint:block:block_rq_issue
{
  @issue[args->rq] = nsecs;
}

tracepoint:block:block_rq_complete
/@issue[args->rq]/
{
  $d = nsecs - @issue[args->rq];
  @blkio_ms = hist($d / 1000000);
  delete(@issue[args->rq]);
}

interval:s:10 { print(@blkio_ms); clear(@blkio_ms); exit(); }
'
```

网络收发“延迟”（用户视角）：`recvfrom()` 阻塞/耗时直方图，这不是“包在网卡/内核里飞行时间”，而是“线程调用 `recvfrom()` 到返回的时间”，非常实用
```
sudo TARGET_TGID=1234 bpftrace -e '
tracepoint:syscalls:sys_enter_recvfrom
/pid == strtol(env("TARGET_TGID"))/
{
  @start[tid] = nsecs;
}

tracepoint:syscalls:sys_exit_recvfrom
/@start[tid]/
{
  $d = nsecs - @start[tid];
  @recv_us = hist($d / 1000);
  delete(@start[tid]);
}

interval:s:10 { print(@recv_us); clear(@recv_us); exit(); }
'
```

Off-CPU 分析（线程为什么不在跑）：按“阻塞点栈”聚合时间（可做火焰图），这个会输出类似 `@[ustack, kstack] = sum(offcpu_time)` 的聚合，适合拿去做 off-CPU 火焰图。

```
sudo TARGET_TGID=1234 bpftrace -e '
tracepoint:sched:sched_switch
/pid == strtol(env("TARGET_TGID"))/
{
  // 当前线程（属于目标进程）要下 CPU：记录开始睡眠时间 + 栈
  @off_ts[tid] = nsecs;
  @off_k[tid] = kstack(20);
  @off_u[tid] = ustack(20);
}

tracepoint:sched:sched_switch
/@off_ts[args->next_pid]/
{
  // 目标线程重新被切上 CPU：计算 off-cpu 时长，并按“当时阻塞栈”聚合
  $t = args->next_pid;
  $d = nsecs - @off_ts[$t];

  @[ @off_u[$t], @off_k[$t] ] = sum($d);

  delete(@off_ts[$t]);
  delete(@off_k[$t]);
  delete(@off_u[$t]);
}

interval:s:10 { print(@); exit(); }
'
```

锁竞争/阻塞点：`futex()` 等待耗时直方图（近似“锁等多久”），很多用户态锁（pthread mutex、某些 runtime 锁）最终会走 futex 等待。

```
sudo TARGET_TGID=1234 bpftrace -e '
tracepoint:syscalls:sys_enter_futex
/pid == strtol(env("TARGET_TGID"))/
{
  @start[tid] = nsecs;
  @stack[tid] = ustack(20);
}

tracepoint:syscalls:sys_exit_futex
/@start[tid]/
{
  $d = nsecs - @start[tid];
  @futex_us = hist($d / 1000);
  @[ @stack[tid] ] = count();  // 也可以按栈统计次数（或改成 sum($d)）
  delete(@start[tid]);
  delete(@stack[tid]);
}

interval:s:10 { print(@futex_us); clear(@futex_us); exit(); }
'
```

资源与行为计数：Top 系统调用（按 PID/comm 维度聚合），统计 10 秒内每种 syscall 发生次数（syscall id 是数字）。

```
sudo bpftrace -e '
tracepoint:raw_syscalls:sys_enter
{
  @[comm, pid, args->id] = count();
}

interval:s:10 { print(@, 20); exit(); }
'
```

内核观测：用户态 page fault 计数/直方图

```
sudo TARGET_TGID=1234 bpftrace -e '
tracepoint:exceptions:page_fault_user
/pid == strtol(env("TARGET_TGID"))/
{
  @pf = count();
}

interval:s:10 { print(@pf); clear(@pf); exit(); }
'
```

内核观测（网络侧常见问题）：TCP 重传计数（定位网络抖动/丢包）

```
sudo bpftrace -e '
tracepoint:tcp:tcp_retransmit_skb
{
  @[comm, pid] = count();
}

interval:s:10 { print(@, 20); exit(); }
'
```

函数级追踪（uprobes/uretprobes）：统计某个用户态函数耗时直方图，假设你要看 `/path/to/bin` 里的 `do_work` 函数（必须能被符号定位到）。

```
sudo bpftrace -e '
uprobe:/path/to/bin:do_work
{
  @ts[tid] = nsecs;
}

uretprobe:/path/to/bin:do_work
/@ts[tid]/
{
  $d = nsecs - @ts[tid];
  @do_work_us = hist($d / 1000);
  delete(@ts[tid]);
}

interval:s:10 { print(@do_work_us); clear(@do_work_us); exit(); }
'
```

对于多实例的优化方向，主要在容器编排、数据传递。这里我们统一放到运维部署的章节。

## 5.典型业务场景实例(nginx + 软件实例 + nfs + kafka + mqtt + postgresql + clickhouse)
我们以汽车领域的车云计算，作为我们的业务示例，来讲一讲在业务场景下，我们的服务端是一个什么样的运行模式，这里主要以一个简单的示意图


## 6.持续集成
此处与行业的方式紧密相关，例如汽车行业的OTA、桌面软件更新、云服务的热更新，针对服务端，我们需要的东西其实很简单和传统，即代码管理和远程自动出包部署。
我们需要的工具包括gitlab\jenkins\ansible，鉴于本人并非专业的运维，而大部分的公司里面，也不需要开发作为运维，更多的时候，我们是为了打造个人的开源项目才需要自动化的cicd，所以我们介绍一下基于github action的持续集成，以及出包之后，利用ansible将安装包自动部署到服务器上。
项目的依赖越是复杂，代码越是庞大，整个过程就越是难搞，所以我们就简单的用hello world来做一个流程通关。
首先，需要确保你的项目可以在本地的物理机或者虚拟机上编译出包，我们这里使用cmake全家桶，把依赖下载、编译、打包全部交给cmake，这里给出一个简单的示例：
```
cmake_minimum_required(VERSION 3.21)
project(fetch_build_package_demo VERSION 0.1.0 LANGUAGES CXX)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

include(FetchContent)

# 让依赖更安静、构建更快（可选）
set(FMT_DOC OFF CACHE BOOL "" FORCE)
set(FMT_TEST OFF CACHE BOOL "" FORCE)

FetchContent_Declare(
  fmt
  GIT_REPOSITORY https://github.com/fmtlib/fmt.git
  GIT_TAG 10.2.1
)
FetchContent_MakeAvailable(fmt)

add_executable(hello_demo src/main.cpp)
target_link_libraries(hello_demo PRIVATE fmt::fmt)
target_compile_features(hello_demo PRIVATE cxx_std_17)

# -------- 安装规则 --------
include(GNUInstallDirs)

install(TARGETS hello_demo
  RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}
)

# -------- 打包（CPack）--------
set(CPACK_PACKAGE_NAME "hello-demo")
set(CPACK_PACKAGE_VENDOR "demo")
set(CPACK_PACKAGE_CONTACT "you@example.com")
set(CPACK_PACKAGE_DESCRIPTION_SUMMARY "Demo: FetchContent + build + install + package")
set(CPACK_PACKAGE_VERSION ${PROJECT_VERSION})

# 生成 .deb 和 .tar.gz（也可以只留一个）
set(CPACK_GENERATOR "DEB;TGZ")

# DEB 相关信息（最基本字段）
set(CPACK_DEBIAN_PACKAGE_MAINTAINER "demo")

include(CPack)
```
之后，我们登入github中，构建一个新的仓库，并关联到我们本地的代码上，构建工作流和action。
```
name: package-on-tag

  

on:

  push:

    tags:

      - "v*"

  workflow_dispatch:

  

permissions:

  contents: write

  

jobs:

  build:

    strategy:

      fail-fast: false

      matrix:

        os: [ubuntu-22.04, windows-2022]

  

    runs-on: ${{ matrix.os }}

  

    steps:

      - uses: actions/checkout@v4

  

      - uses: lukka/get-cmake@latest

        with:

          cmakeVersion: 3.27.9

  

      - name: Configure

        run: cmake -S . -B build -DCMAKE_BUILD_TYPE=Release

  

      - name: Build (Linux)

        if: runner.os == 'Linux'

        run: cmake --build build -j

  

      - name: Build (Windows)

        if: runner.os == 'Windows'

        run: cmake --build build --config Release

  

      - name: Package with CPack (Linux)

        if: runner.os == 'Linux'

        run: |

          cd build

          cpack -G TGZ

          cpack -G DEB

  

      - name: Package with CPack (Windows)

        if: runner.os == 'Windows'

        run: |

          cd build

          cpack -G ZIP

  

      - name: Upload assets to GitHub Release

        if: startsWith(github.ref, 'refs/tags/')

        uses: softprops/action-gh-release@v2

        with:

          files: |

            build/*.tar.gz

            build/*.deb

            build/*.zip
```
设置完触发条件之后（设置为推送tag），我们就可以尝试使用这个流程走程序包的构建。
完成出包之后，我们这里在服务器中下载这个release，并使用ansible推送到目标服务器完成安装。
具体的步骤如下：


## 7.运维部署（docker swarm | k8s）
本机的服务应用，我们可以直接以安装包的形式安装运行，云端的服务，我们将使用docker打包环境后，使用容器编排技术运行。
我们这里用swarm做一个简单的示例：
